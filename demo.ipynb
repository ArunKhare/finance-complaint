{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'model_trainer_ref_artifact': {'trained_model_file_path': '/home/avnish/iNeuron_Private_Intelligence_Limited/project_neuron/ml/finance-complaint/finance_artifact/model_trainer/20221114_133246/trained_model/finance_estimator', 'label_indexer_model_file_path': '/home/avnish/iNeuron_Private_Intelligence_Limited/project_neuron/ml/finance-complaint/finance_artifact/model_trainer/20221114_133246/label_indexer'}, 'model_trainer_train_metric_artifact': {'f1_score': 0.8323223186625319, 'precision_score': 0.8107160851330072, 'recall_score': 0.8618937644341801}, 'model_trainer_test_metric_artifact': {'f1_score': 0.8610711544640307, 'precision_score': 0.8400419311495159, 'recall_score': 0.8849557522123893}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.entity.artifact_entity import ModelTrainerArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelTrainerArtifact.construct_object(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_trainer_ref_artifact': {'trained_model_file_path': '/home/avnish/iNeuron_Private_Intelligence_Limited/project_neuron/ml/finance-complaint/finance_artifact/model_trainer/20221114_133246/trained_model/finance_estimator',\n",
       "  'label_indexer_model_file_path': '/home/avnish/iNeuron_Private_Intelligence_Limited/project_neuron/ml/finance-complaint/finance_artifact/model_trainer/20221114_133246/label_indexer'},\n",
       " 'model_trainer_train_metric_artifact': {'f1_score': 0.8323223186625319,\n",
       "  'precision_score': 0.8107160851330072,\n",
       "  'recall_score': 0.8618937644341801},\n",
       " 'model_trainer_test_metric_artifact': {'f1_score': 0.8610711544640307,\n",
       "  'precision_score': 0.8400419311495159,\n",
       "  'recall_score': 0.8849557522123893}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._asdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.entity.config_entity import PredictionPipelineConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pipeline_config = PredictionPipelineConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_dir': 'finance_data/input/finance_complaint',\n",
       " 'prediction_dir': 'finance_data/prediction/20221120_105141',\n",
       " 'failed_dir': 'finance_data/failed/20221120_105141',\n",
       " 'archive_dir': 'finance_data/archive/20221120_105141',\n",
       " 'region_name': 'ap-south-1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_pipeline_config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.constant.prediction_pipeline_config.file_config import S3_DATA_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finance-cat-service'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_DATA_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_file_path = \"s3n://finance-cat-service/finance_data/input/finance_complaint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"/home/avnish/iNeuron_Private_Intelligence_Limited/project_neuron/ml/finance-complaint/finance_artifact/data_ingestion/feature_store/finance_complaint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a spark_session\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfinance_complaint\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspark_manager\u001b[39;00m \u001b[39mimport\u001b[39;00m spark_session\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\finance_complaint\\config\\spark_manager.py:18\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# spark = SparkSession.builder.master('local[*]').appName('finance_complaint') .getOrCreate()\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# hadoop_conf = spark._jsc.hadoopConfiguration()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# spark_session=spark\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating a spark_session\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m spark_session \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mmaster(\u001b[39m'\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mappName(\u001b[39m'\u001b[39;49m\u001b[39mfinance_complaint\u001b[39;49m\u001b[39m'\u001b[39;49m) \\\n\u001b[0;32m     19\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.executor.instances\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m     20\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.executor.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m6g\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m     21\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.driver.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m6g\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m     22\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.executor.memoryOverhead\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m8g\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m     23\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m'\u001b[39;49m\u001b[39mspark.jars.packages\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mcom.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m     24\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m     25\u001b[0m     \u001b[39m# \u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSPARK SESSION CREATED\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    229\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 392\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 339\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    340\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\java_gateway.py:101\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpopen_kwargs)\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[39m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpopen_kwargs)\n\u001b[0;32m    103\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m    855\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m    856\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m--> 858\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m    859\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m    860\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m    861\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m    862\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m    863\u001b[0m                         errread, errwrite,\n\u001b[0;32m    864\u001b[0m                         restore_signals, start_new_session)\n\u001b[0;32m    865\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\subprocess.py:1311\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1311\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1312\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1313\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1314\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1315\u001b[0m                              creationflags,\n\u001b[0;32m   1316\u001b[0m                              env,\n\u001b[0;32m   1317\u001b[0m                              cwd,\n\u001b[0;32m   1318\u001b[0m                              startupinfo)\n\u001b[0;32m   1319\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1320\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1327\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1328\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "from finance_complaint.config.spark_manager import spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.parquet(file_path).limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.entity.estimator import S3FinanceEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.constant.model import S3_MODEL_BUCKET_NAME, S3_MODEL_DIR_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('finance-cat-service', 'model-registry')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_MODEL_BUCKET_NAME,S3_MODEL_DIR_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n"
     ]
    }
   ],
   "source": [
    "estimator = S3FinanceEstimator(bucket_name=S3_MODEL_BUCKET_NAME, s3_key=S3_MODEL_DIR_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = estimator.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------+-----------+----------+----------------------------+-----------------+\n",
      "|scaled_input_features|rawPrediction|probability|prediction|prediction_consumer_disputed|consumer_disputed|\n",
      "+---------------------+-------------+-----------+----------+----------------------------+-----------------+\n",
      "| (52,[0,4,7,22,23,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,4,7,20,23,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,4,7,18,22,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,4,7,22,23,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,3,7,22,23,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,5,9,15,17,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,3,7,22,39,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,3,7,22,23,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,3,7,33,36,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[2,3,7,13,21,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,3,7,28,34,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,3,7,22,32,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,3,7,15,17,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[2,4,7,22,39,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,4,7,11,22,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,4,7,28,34,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,4,7,22,23,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,4,7,11,28,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[1,4,7,15,17,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "| (52,[0,3,7,15,17,...|       [20.0]|      [1.0]|       0.0|                         N/A|              N/A|\n",
      "+---------------------+-------------+-----------+----------+----------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([ 'scaled_input_features',\n",
    " 'rawPrediction',\n",
    " 'probability',\n",
    " 'prediction',\n",
    " 'prediction_consumer_disputed','consumer_disputed']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.constant.environment.variable_key import AWS_ACCESS_KEY_ID_ENV_KEY,AWS_SECRET_ACCESS_KEY_ENV_KEY\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "access_key_id = os.getenv(AWS_ACCESS_KEY_ID_ENV_KEY, )\n",
    "secret_access_key = os.getenv(AWS_SECRET_ACCESS_KEY_ENV_KEY, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ss \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mmaster(\u001b[39m'\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mappName(\u001b[39m'\u001b[39;49m\u001b[39mfinance_complaint\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m      2\u001b[0m     \u001b[39m# .config(\"spark.executor.instances\",'1')\\\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m     \u001b[39m# .config(\"spark.executor.memory\",\"6g\")\\\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39m# .config('spark.driver.memory',\"6g\") \\\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    229\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 392\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 339\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    340\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\site-packages\\pyspark\\java_gateway.py:101\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpopen_kwargs)\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[39m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpopen_kwargs)\n\u001b[0;32m    103\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m    855\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m    856\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m--> 858\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m    859\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m    860\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m    861\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m    862\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m    863\u001b[0m                         errread, errwrite,\n\u001b[0;32m    864\u001b[0m                         restore_signals, start_new_session)\n\u001b[0;32m    865\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\arunk\\Big-Data\\Projects\\finance-complaint\\env\\lib\\subprocess.py:1311\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1311\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1312\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1313\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1314\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1315\u001b[0m                              creationflags,\n\u001b[0;32m   1316\u001b[0m                              env,\n\u001b[0;32m   1317\u001b[0m                              cwd,\n\u001b[0;32m   1318\u001b[0m                              startupinfo)\n\u001b[0;32m   1319\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1320\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1327\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1328\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "ss = SparkSession.builder.master('local[*]').appName('finance_complaint').getOrCreate()\n",
    "    # .config(\"spark.executor.instances\",'1')\\\n",
    "    \n",
    "    # .config(\"spark.executor.memory\",\"6g\")\\\n",
    "    # .config('spark.driver.memory',\"6g\") \\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.Builder.appName of <pyspark.sql.session.SparkSession.Builder object at 0x00000239F5B167F0>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014dc64d5a83f8ce05be716c029c98cce2a6ab30ae5e20573055dca4a5c17f1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
